
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Call for Papers</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
  <style>
  body {
    font-family: 'Roboto', sans-serif;
    margin: 0;
    padding: 0;
    background: url('neural-network-8684318_1280.jpg') no-repeat center center fixed;
    background-size: cover;
    color: #111;
    backdrop-filter: brightness(1.05) blur(2px);
  }
  header, main, nav {
    max-width: 960px;
    margin: auto;
    padding: 20px;
  }
  nav {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
  }
  nav a {
    text-decoration: none;
    color: #0077cc;
    font-weight: bold;
    background: rgba(255, 255, 255, 0.8);
    padding: 5px 10px;
    border-radius: 5px;
    flex: 1 0 auto;
  }
  h1, h2 {
    color: #2c3e50;
    background: rgba(255, 255, 255, 0.8);
    padding: 10px;
    border-radius: 5px;
  }
  main {
    background: rgba(255, 255, 255, 0.85);
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 0 10px rgba(0,0,0,0.1);
  }
  @media screen and (max-width: 600px) {
    nav {
      flex-direction: column;
      gap: 8px;
    }
    nav a {
      flex: 1 0 100%;
      text-align: center;
    }
  }
</style>

<style>
    .nav-line {
      display: flex;
      justify-content: space-between;
      padding: 10px 20px;
      background-color: #f0f0f0;
      font-weight: bold;
    }

    .nav-line a {
      text-decoration: none;
      color: #007bff;
    }

  </style>


</head>
<body>
<header>
    <h1>Computing with Deep Neural Networks in Distributed Framework</h1>
    
  </header>
  <nav>
    <a href="cfp.html">Call for Papers</a>
    <a href="program.html">Workshop Program</a>
    <a href="papers.html">Accepted Papers</a>
    <a href="registration.html">Registration Details</a>
    <a href="submission.html">Submission</a>
    <a href="committee.html">Committee</a>
  </nav>

  <main>
    <h2>Call for Papers</h2>

  <p style="font-size: 20px; text-align: center;">
    Workshop titled <br>
    <strong style="color: #f54242;">“Computing with Deep Neural Networks in Distributed Framework”</strong> <br>
    as a part of the <strong style="color: #4842f5;">27th International Conference on Distributed Computing and Networking</strong>,<br>
    to be held in <strong style="color: #f5429b;">Nara, Japan, during January 6–9, 2026</strong>
  </p>
  
<div class="nav-line">
    <a href="#important-dates">Important Dates</a>
    <a href="#contact-us">Contact Us</a>
  </div>

  <p style="text-align: justify;">
    Deep Neural Networks (DNNs) exhibit high efficiency and wide applicability in various domains like image classification, image recognition, language translation, language modeling, video captioning, speech recognition, and recommendation systems, among others. Due to its wide applicability, DNNs have grown massively in size to capture more complex scenarios, resulting in considerably more complex computations during training. Nowadays, improved data generation technologies with low cost cause an exponential growth of data, which attributes the increased model size with enhanced capabilities. With the emergence of more nonlinear and high-dimensional datasets, there is a growing need to effectively train the deeper and increasingly complex DNN architectures. As a result, the DNN training process becomes so expensive in terms of time and resources that it exceeds the capacity of single accelerator. It is observed that with sufficient resources, DNNs can achieve dramatic improvements in its performance. A trained DNN is often used for inference which is not such costly task like training. The massive surge in resource requirements for DNN training has led to new devices such as Google’s TPU, NVidia’s Tesla, or Xilinx Alveo in addition to custom accelerators. Due to lack of cost-effectiveness of those high-end servers, parallelization of the training process over multiple commodity hardware, like small workstations, PCs, and Laptops, has become relevant, which necessitates efficient parallel strategies for distributed training on large clusters.
  </p>

  <p style="text-align: justify;">
The purpose of this Workshop is to provide a leading-edge forum for academics as well as industrial professionals involved in the field to contribute for disseminating the most innovative research and developments of all aspects of distributed and parallel neural computing. The workshop will discuss several new and challenging issues in this field that require new technological innovations. We welcome submissions that identify challenges, propose new framework, architecture, and algorithms, and provide the detailed explanations of how the contributions advance the state-of-the-art in distributed DNN training, reports experimental results in comparison with the state-of-the-art systems. The workshop will focus on the distribution and parallelization of the neural computing models across accelerators including (but not limited to) the following aspects: 
  </p>

  <ul>
    <li>Parallel and distributed algorithms for neural network training and their implementations</li>
    <li>Scalability of algorithms and data structures for parallel and distributed neural computing systems</li>
    <li>Communication and synchronization protocols, resource management (scheduling and load-balancing) for parallel and distributed neural computing systems</li>
    <li>Novel computer architecture for parallel and distributed neural computing systems including (but not limited to) data-level and model-level parallelisms, their design, analysis, implementation</li>
    <li>Fault resilience and performance measurements of multiple-accelerator systems, and homogeneous and heterogeneous multiple single-accelerator systems</li>
    <li>Parallel and distributed software, programming environments, and tools for neural computing</li>
    <li>Applications of parallel and distributed neural computing including (but not limited to) in big data analytics, management of big data, cloud computing</li>
  </ul>

  <h2>Guidelines for Authors</h2>
    <ol>
      <li style="text-align: justify;">Please check the ACM policy on Authorship (<a href="https://www.acm.org/publications/policies/new-acm-policy-on-authorship">https://www.acm.org/publications/policies/new-acm-policy-on-authorship</a>) and the use of generative AI in the papers (<a href="https://www.acm.org/publications/policies/frequently-asked-questions">https://www.acm.org/publications/policies/frequently-asked-questions</a>). Please ensure that the same is being followed in the accepted papers. Please note that authorships cannot be added or removed once a paper is accepted.</li>
      <li style="text-align: justify;">The submitted manuscripts will undergo peer review following ACM Peer Review Policy (<a href="https://www.acm.org/publications/policies/peer-review" target="_blank">https://www.acm.org/publications/policies/peer-review</a>). The accepted papers will be published in a companion volume along with the main conference proceedings, as stated below. At least one author of an accepted paper must register and present their paper at the workshop in person.</li>
    </ol>




  <h2>New Open Access Publishing Model for ACM</h2>

  <p style="text-align: justify;">
ICDCN 2026 workshop proceedings will be published as a companion volume along with the main conference proceedings. Note that, ACM has introduced a new open access publishing model for the International Conference Proceedings Series (ICPS). Please check the ACM Open Publication Model (<a href="https://www.acm.org/publications/icps/faq">https://www.acm.org/publications/icps/faq</a>). Authors based at institutions that are not yet part of the ACM Open program ("<a href="https://libraries.acm.org/acmopen/open-participants">https://libraries.acm.org/acmopen/open-participants</a>) and do not qualify for a waiver will be required to pay an article processing charge (APC) to publish their ICPS article in the ACM Digital Library. Please note that this APC is in addition to the conference registration fee and is handled by the ACM directly. To determine whether or not an APC will be applicable to your article, please follow the detailed guidance here: <a href="https://www.acm.org/publications/icps/author-guidance">https://www.acm.org/publications/icps/author-guidance</a>. For any clarifications, please contact <a href="mailto:icps-info@acm.org">icps-info@acm.org</a>. 
  </p>

  

<div id="important-dates" class="section">
    <h2>Timeline</h2>
  <ul>
    <li>Deadline for Workshop Papers: <strong>September 15, 2025</strong></li>
    <li>Decision on Acceptance/Rejection of the Workshop Papers: <strong>October 25, 2025</strong></li>
    <li>Camera-Ready Papers: <strong>November 15, 2025</strong> (hard deadline)</li>
  </ul>
  </div>

  <div id="contact-us" class="section">
    <h2>Contact Us</h2>
    <p><strong>Professor Rajat Kumar De</strong><br>
       Indian Statistical Institute, Kolkata<br>
       Email: rajat@isical.ac.in / rajatkde@gmail.com<br>
       Phone: +91 9433008009, +91 9088015909</p>

<p><strong>Professor Nabendu Chaki</strong><br>
       University of Calcutta, Kolkata<br>
       Email: nchaki@gmail.com<br>
       Phone: +91 9433068073</p>
  </div>

  </main>
</body>
</html>
