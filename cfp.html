
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Call for Papers</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
  <style>
  body {
    font-family: 'Roboto', sans-serif;
    margin: 0;
    padding: 0;
    background: url('neural-network-8684318_1280.jpg') no-repeat center center fixed;
    background-size: cover;
    color: #111;
    backdrop-filter: brightness(1.05) blur(2px);
  }
  header, main, nav {
    max-width: 960px;
    margin: auto;
    padding: 20px;
  }
  nav {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
  }
  nav a {
    text-decoration: none;
    color: #0077cc;
    font-weight: bold;
    background: rgba(255, 255, 255, 0.8);
    padding: 5px 10px;
    border-radius: 5px;
    flex: 1 0 auto;
  }
  h1, h2 {
    color: #2c3e50;
    background: rgba(255, 255, 255, 0.8);
    padding: 10px;
    border-radius: 5px;
  }
  main {
    background: rgba(255, 255, 255, 0.85);
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 0 10px rgba(0,0,0,0.1);
  }
  @media screen and (max-width: 600px) {
    nav {
      flex-direction: column;
      gap: 8px;
    }
    nav a {
      flex: 1 0 100%;
      text-align: center;
    }
  }
</style>

<style>
    .nav-line {
      display: flex;
      justify-content: space-between;
      padding: 10px 20px;
      background-color: #f0f0f0;
      font-weight: bold;
    }

    .nav-line a {
      text-decoration: none;
      color: #007bff;
    }

  </style>


</head>
<body>
<header>
    <h1>Computing with Deep Neural Networks in Distributed Framework</h1>
    
  </header>
  <nav>
    <a href="index.html">Home</a>
    <a href="cfp.html">Call for Papers</a>
    <a href="program.html">Workshop Program</a>
    <a href="papers.html">Accepted Papers</a>
    <a href="registration.html">Registration Details</a>
    <a href="submission.html">Submission</a>
    <a href="committee.html">Committee</a>
  </nav>

  <main>
    <h2>Call for Papers</h2>

  <p style="font-size: 16px; text-align: center;">
    Workshop titled <br>
    “Computing with Deep Neural Networks in Distributed Framework” as a part of the <br>
    27th International Conference on Distributed Computing and Networking,<br>
    to be held in Nara, Japan, during January 6–9, 2026
  </p>
  
<div class="nav-line">
    <a href="#important-dates">Important Dates</a>
    <a href="#contact-us">Contact Us</a>
  </div>

  <p style="text-align: justify;">
    Deep Neural Networks (DNNs) exhibit high efficiency and wide applicability in various domains like image classification, image recognition, language translation, language modeling, video captioning, speech recognition, and recommendation systems, among others. Due to its wide applicability, DNNs have grown massively in size to capture more complex scenarios, resulting in considerably more complex computations during training. Nowadays, improved data generation technologies with low cost cause an exponential growth of data, which attributes the increased model size with enhanced capabilities. With the emergence of more nonlinear and high-dimensional datasets, there is a growing need to effectively train the deeper and increasingly complex DNN architectures. As a result, the DNN training process becomes so expensive in terms of time and resources that it exceeds the capacity of single accelerator. It is observed that with sufficient resources, DNNs can achieve dramatic improvements in its performance. A trained DNN is often used for inference which is not such costly task like training. The massive surge in resource requirements for DNN training has led to new devices such as Google’s TPU, NVidia’s Tesla, or Xilinx Alveo in addition to custom accelerators. Due to lack of cost-effectiveness of those high-end servers, parallelization of the training process over multiple commodity hardware, like small workstations, PCs, and Laptops, has become relevant, which necessitates efficient parallel strategies for distributed training on large clusters.
  </p>

  <p style="text-align: justify;">
The purpose of this Workshop is to provide a leading-edge forum for academics as well as industrial professionals involved in the field to contribute for disseminating the most innovative research and developments of all aspects of distributed and parallel neural computing. The workshop will discuss several new and challenging issues in this field that require new technological innovations. We welcome submissions that identify challenges, propose new framework, architecture, and algorithms, and provide the detailed explanations of how the contributions advance the state-of-the-art in distributed DNN training, reports experimental results in comparison with the state-of-the-art systems. The workshop will focus on the distribution and parallelization of the neural computing models across accelerators including (but not limited to) the following aspects: 
  </p>

  <ul>
    <li>Parallel and distributed algorithms for neural network training and their implementations</li>
    <li>Scalability of algorithms and data structures for parallel and distributed neural computing systems</li>
    <li>Communication and synchronization protocols, resource management (scheduling and load-balancing) for parallel and distributed neural computing systems</li>
    <li>Novel computer architecture for parallel and distributed neural computing systems including (but not limited to) data-level and model-level parallelisms, their design, analysis, implementation</li>
    <li>Fault resilience and performance measurements of multiple-accelerator systems, and homogeneous and heterogeneous multiple single-accelerator systems</li>
    <li>Parallel and distributed software, programming environments, and tools for neural computing</li>
    <li>Applications of parallel and distributed neural computing including (but not limited to) in big data analytics, management of big data, cloud computing</li>
  </ul>

  <p style="text-align: justify;">

The submitted manuscripts will undergo peer review following ACM Peer Review Policy (<a href="https://www.acm.org/publications/policies/peer-review" target="_blank">https://www.acm.org/publications/policies/peer-review</a>). The accepted papers will be published in a companion volume along with the main conference proceedings, as stated below. At least one author of an accepted paper must register and present their paper at the workshop in person.
  </p>

  <h2>New Open Access Publishing Model for ACM</h2>

  <p style="text-align: justify;">
ICDCN 2026 workshop proceedings will be published as a companion volume along with the main conference proceedings. However, please note that ACM has moved to a new open access publishing model for all conference proceedings to be published via ACM ICPS. The authors have to pay an Article Processing Charge (APC) to ACM (which is beyond the regular conference registration fee) if the corresponding author’s organization is not a member of the ACM Open program. Please check the details through this link: <a href="https://www.acm.org/publications/icps/author-guidance">https://www.acm.org/publications/icps/author-guidance</a>. Several institutes across the globe are already members of the ACM Open program. The authors can use the following link to check if their organization is a member under the ACM Open program: <a href="https://libraries.acm.org/acmopen/open-participants">https://libraries.acm.org/acmopen/open-participants</a>. For any clarifications, please contact <a href="mailto:icps-info@acm.org">icps-info@acm.org</a>. 
  </p>

  

<div id="important-dates" class="section">
    <h2>Timeline</h2>
  <ul>
    <li>Deadline for Workshop Papers: <strong>September 15, 2025</strong></li>
    <li>Decision on Acceptance/Rejection of the Workshop Papers: <strong>October 25, 2025</strong></li>
    <li>Camera-Ready Papers: <strong>November 15, 2025</strong> (hard deadline)</li>
  </ul>
  </div>

  <div id="contact-us" class="section">
    <h2>Contact Us</h2>
    <p><strong>Prof. Rajat Kumar De</strong><br>
       Indian Statistical Institute, Kolkata<br>
       Email: rajat@isical.ac.in / rajatkde@gmail.com<br>
       Phone: +91 9433008009, +91 9088015909</p>

<p><strong>Prof. Nabendu Chaki</strong><br>
       University of Calcutta, Kolkata<br>
       Email: nchaki@gmail.com<br>
       Phone: +91 9433068073</p>
  </div>

  </main>
</body>
</html>